from pymilvus import MilvusClient, DataType, Function, FunctionType
import numpy as np
import concurrent.futures
from pymilvus import AnnSearchRequest
from pymilvus import RRFRanker
import torch
from reranker import text_rerank
import json

client = MilvusClient(uri="http://127.0.0.1:19530")

class MilvusColbertRetriever:
    def __init__(self, milvus_client, collection_name, dim=128):
        # Initialize the retriever with a Milvus client, collection name, and dimensionality of the vector embeddings.
        # If the collection exists, load it.
        self.collection_name = collection_name
        self.client = milvus_client
        if self.client.has_collection(collection_name=self.collection_name):
            self.client.load_collection(collection_name)
        self.dim = dim

    def create_collection(self):
        # Create a new collection in Milvus for storing embeddings.
        # Drop the existing collection if it already exists and define the schema for the collection.
        if self.client.has_collection(collection_name=self.collection_name):
            self.client.drop_collection(collection_name=self.collection_name)
        schema = self.client.create_schema(
            auto_id=True,
            enable_dynamic_fields=True,
        )
        schema.add_field(field_name="pk", datatype=DataType.INT64, is_primary=True)
        schema.add_field(
            field_name="text", 
            datatype=DataType.VARCHAR, 
            max_length=20000, 
            enable_analyzer=True, 
            description="raw text of product description"
        )
        schema.add_field(
            field_name="text_dense", 
            datatype=DataType.FLOAT_VECTOR, 
            dim=1024, 
            description="text dense embedding"
        )
        schema.add_field(
            field_name="text_sparse", 
            datatype=DataType.SPARSE_FLOAT_VECTOR, 
            description="text sparse embedding auto-generated by the built-in BM25 function"
        )
        schema.add_field(
            field_name="image_dense", 
            datatype=DataType.FLOAT_VECTOR, 
            dim=128,
            description="image dense embedding"
        )
        schema.add_field(field_name="seq_id", datatype=DataType.INT16)
        schema.add_field(field_name="doc_id", datatype=DataType.INT64)
        schema.add_field(field_name="doc", datatype=DataType.VARCHAR, max_length=65535)
        schema.add_field(field_name="customName", datatype=DataType.VARCHAR, max_length=65535)

        # Add function to schema
        bm25_function = Function(
            name="text_bm25_emb",
            input_field_names=["text"],
            output_field_names=["text_sparse"],
            function_type=FunctionType.BM25,
        )
        schema.add_function(bm25_function)
        
        self.client.create_collection(
            collection_name=self.collection_name, schema=schema
        )

    def create_index(self):
        # Create an index on the vector field to enable fast similarity search.
        # Releases and drops any existing index before creating a new one with specified parameters.
        self.client.release_collection(collection_name=self.collection_name)
        self.client.drop_index(
            collection_name=self.collection_name, index_name="vector"
        )
        index_params = self.client.prepare_index_params()
        index_params.add_index(
            field_name="text_dense",
            index_name="text_dense_index",
            index_type="AUTOINDEX",
            metric_type="IP"
        )
        index_params.add_index(
            field_name="text_sparse",
            index_name="text_sparse_index",
            index_type="SPARSE_INVERTED_INDEX",
            metric_type="BM25",
            params={"inverted_index_algo": "DAAT_MAXSCORE"}, # or "DAAT_WAND" or "TAAT_NAIVE"
        )
        index_params.add_index(
            field_name="image_dense",
            index_name="image_dense_index",
            index_type="HNSW",  # or any other index type you want
            metric_type="IP",  # or the appropriate metric type
            params={
                "M": 16,
                "efConstruction": 500,
            },  # adjust these parameters as needed
        )

        self.client.create_index(
            collection_name=self.collection_name, index_params=index_params, sync=True
        )

    # def create_scalar_index(self):
    #     # Create a scalar index for the "doc_id" field to enable fast lookups by document ID.
    #     self.client.release_collection(collection_name=self.collection_name)

    #     index_params = self.client.prepare_index_params()
    #     index_params.add_index(
    #         field_name="doc_id",
    #         index_name="int32_index",
    #         index_type="INVERTED",  # or any other index type you want
    #     )

    #     self.client.create_index(
    #         collection_name=self.collection_name, index_params=index_params, sync=True
    #     )
    
    def search(self, data, topk):
        # Perform a vector search on the collection to find the top-k most similar documents.
        search_params = {"metric_type": "IP", "params": {}}
        results = self.client.search(
            self.collection_name,
            data,
            limit=int(50),
            output_fields=["vector", "seq_id", "doc_id"],
            search_params=search_params,
        )
        doc_ids = set()
        for r_id in range(len(results)):
            for r in range(len(results[r_id])):
                doc_ids.add(results[r_id][r]["entity"]["doc_id"])

        scores = []

        def rerank_single_doc(doc_id, data, client, collection_name):
            # Rerank a single document by retrieving its embeddings and calculating the similarity with the query.
            doc_colbert_vecs = client.query(
                collection_name=collection_name,
                filter=f"doc_id in [{doc_id}]",
                output_fields=["seq_id", "vector", "doc"],
                limit=1000,
            )
            doc_vecs = np.vstack(
                [doc_colbert_vecs[i]["vector"] for i in range(len(doc_colbert_vecs))]
            )
            score = np.dot(data, doc_vecs.T).max(1).sum()
            return (score, doc_id)

        with concurrent.futures.ThreadPoolExecutor(max_workers=300) as executor:
            futures = {
                executor.submit(
                    rerank_single_doc, doc_id, data, client, self.collection_name
                ): doc_id
                for doc_id in doc_ids
            }
            for future in concurrent.futures.as_completed(futures):
                score, doc_id = future.result()
                scores.append((score, doc_id))

        scores.sort(key=lambda x: x[0], reverse=True)
        if len(scores) >= topk:
            return scores[:topk]
        else:
            return scores

    def Img_search(self, data,customNames, topk,doc_id=[]):
        # Perform a vector search on the collection to find the top-k most similar documents.
        # data是一个向量组，这里在进行批量检索
        try: 
            if(doc_id != []):
                results = self.client.search(
                    self.collection_name,
                    data,
                    limit=int(topk*1023),
                    anns_field="image_dense",
                    filter=f'customName in {customNames} and doc_id in {doc_id}',
                    output_fields=["image_dense", "seq_id", "doc_id","customName"],
                    search_params={"metric_type": "IP"}
                )
            else:
                results = self.client.search(
                    self.collection_name,
                    data,
                    limit=int(topk*1023),
                    anns_field="image_dense",
                    filter=f'customName in {customNames}',
                    output_fields=["image_dense", "seq_id", "doc_id","customName"],
                    search_params={"metric_type": "IP"}
                )
        except Exception as e:
            print("colpali检索失败：\n")
            print(str(e))
                 
        docs = []
        seen = set()  # 用于跟踪已见的唯一标识

        for r_id in range(len(results)):
            for r in range(len(results[r_id])):
                # 提取文档的唯一标识组合
                doc_id = results[r_id][r]["entity"]["doc_id"]
                custom_name = results[r_id][r]["entity"]["customName"]
                unique_key = (doc_id, custom_name)  # 创建不可变的唯一键
                
                # 仅当未出现过时才添加到列表
                if unique_key not in seen:
                    seen.add(unique_key)
                    docs.append({
                        "doc_id": doc_id,
                        "customName": custom_name
                    })
        scores = []
        def rerank_single_doc(doc, data, client, collection_name):
            # Rerank a single document by retrieving its embeddings and calculating the similarity with the query.
            doc_id = doc["doc_id"]
            customName = doc["customName"]
            doc_colbert_vecs = client.query(
                collection_name=collection_name,
                filter=f'doc_id == {doc_id} and customName == "{customName}"',
                output_fields=["seq_id", "image_dense", "doc"]
            )
            doc_vecs = np.vstack(
                [doc_colbert_vecs[i]["image_dense"] for i in range(len(doc_colbert_vecs))]
            )
            score = np.dot(data, doc_vecs.T).max(1).sum()
            docPath=""
            for item in doc_colbert_vecs:
                if item["seq_id"] == 0:
                    docPath = item["doc"]
                    break 
            return (score, doc_id, docPath)

        with concurrent.futures.ThreadPoolExecutor(max_workers=300) as executor:
            futures = {
                executor.submit(
                    rerank_single_doc, doc, data, client, self.collection_name
                ): doc
                for doc in docs
            }
            for future in concurrent.futures.as_completed(futures):
                score, doc_id, doc = future.result()
                scores.append((score, doc_id, doc))
 
        scores.sort(key=lambda x: x[0], reverse=True)
        # return scores
        if len(scores) >= topk:
            return scores[:topk]
        else:
            return scores
        
        
    def Muti_hybrid_search(self,query_param, topk, rerank_topn=50, weights=(0.4, 0.3, 0.3)):
            # text semantic search (dense)
            customNames = query_param["customNames"]
            
            count = self.count_entity_customNames(customNames)
            if(count >= rerank_topn*2):
                rerank_topn = rerank_topn
            elif(count < rerank_topn*2 and count > topk*2):
                rerank_topn = count//2
            elif(count < topk*2 and count >= topk):
                rerank_topn = topk
            else:
                topk = count
                rerank_topn =count
            # print(f"topk:{topk}\ncount:{count}\n")    
            search_param_1 = {
                "data": [query_param["text_dense_vector"]],
                "anns_field": "text_dense",
                "param": {"nprobe": 10},
                "expr": f"seq_id == 0 and customName in {customNames}",
                "limit": rerank_topn
            }
            request_1 = AnnSearchRequest(**search_param_1)
            
            # full-text search (sparse)
            search_param_2 = {
                "data": [query_param["text_query"]],
                "anns_field": "text_sparse",
                "param": {"drop_ratio_search": 0.2},
                "expr": f"seq_id == 0 and customName in {customNames}",
                "limit": rerank_topn
            }
            request_2 = AnnSearchRequest(**search_param_2)
            
            request_3 = self.Img_search(query_param["image_query"],customNames,topk)
            
            
            #先混合文本检索
            RRFranker = RRFRanker(100)
            reqs = [request_1,request_2]
            res = client.hybrid_search(
                collection_name=self.collection_name,
                reqs=reqs,
                ranker=RRFranker,
                limit=topk,
                output_fields=["doc"]
            )
            
            #将图片特征点匹配结果与前面已混合的文本检索,提取每页图片的存储位置做并集，存入docs_res
            docs_res=[]
            seen = set()  # 用于跟踪已见的唯一标识
            for hits in res:
                for hit in hits:
                    doc_t=hit["entity"]["doc"]
                    unique_key = (doc_t)  # 创建不可变的唯一键
                    # 仅当未出现过时才添加到列表
                    if unique_key not in seen:
                        seen.add(unique_key)
                        docs_res.append(doc_t)

            for item in request_3:
                doc_img=item[2]
                unique_key = (doc_img)
                if unique_key not in seen:
                        seen.add(unique_key)
                        docs_res.append(doc_img)
        
        
        #每页图的存储位置一定是一个唯一值，根据存储位置匹配数据库中的每一页的caption文本
            doc_texts = client.query(
                    collection_name=self.collection_name,
                    filter=f"doc in {docs_res} and customName in {customNames}",
                    output_fields=["text","doc"],
                    limit=len(docs_res),
                )    
            
            #构建model-rerank的请求，documents的索引顺序就是doc_texts里的顺序
            documents=[]
            for item in doc_texts:
                documents.append(item["text"])
            res = text_rerank(documents,query_param["text_query"],topk)
            
            
            #获取响应中的重排的索引顺序，匹配对应索引顺序的图片存储位置并返回
            search_output = []
            for res_item in res:
                search_output.append(doc_texts[res_item["index"]]["doc"])
            
            if(len(search_output) != topk):
                print(f"向量查询出现问题，不足topk:\n{search_output}")
            return search_output

    
    def Muti_hybrid_search_intersection(self,query_param, topk, rerank_topn=50, weights=(0.4, 0.3, 0.3)):
        # text semantic search (dense)
        customNames = query_param["customNames"]
        
        count = self.count_entity_customNames(customNames)
        if(count >= rerank_topn*2):
            rerank_topn = rerank_topn
        elif(count < rerank_topn*2 and count > topk*2):
            rerank_topn = count//2
        elif(count < topk*2 and count >= topk):
            rerank_topn = topk
        else:
            topk = count
            rerank_topn =count
        # print(f"topk:{topk}\nrerank_topn:{rerank_topn}\ncount:{count}\n")        
        search_param_1 = {
            "data": [query_param["text_dense_vector"]],
            "anns_field": "text_dense",
            "param": {"nprobe": 10},
            "expr": f"seq_id == 0 and customName in {customNames}",
            "limit": rerank_topn
        }
        request_1 = AnnSearchRequest(**search_param_1)
        
        # full-text search (sparse)
        search_param_2 = {
            "data": [query_param["text_query"]],
            "anns_field": "text_sparse",
            "param": {"drop_ratio_search": 0.2},
            "expr": f"seq_id == 0 and customName in {customNames}",
            "limit": rerank_topn
        }
        request_2 = AnnSearchRequest(**search_param_2)
        
        # if(topk*2 > count):
        #     request_3 = self.Img_search(query_param["image_query"],customNames,count)
        # else:
        #     request_3 = self.Img_search(query_param["image_query"],customNames,topk*2)
        request_3 = self.Img_search(query_param["image_query"],customNames,rerank_topn)
        #先混合文本检索
        RRFranker = RRFRanker(100)
        reqs = [request_1,request_2]
        res = client.hybrid_search(
            collection_name=self.collection_name,
            reqs=reqs,
            ranker=RRFranker,
            limit=rerank_topn,
            output_fields=["doc"]
        )
        
        docs_t=[]
        for resItem in res:
            for item in resItem:
                docs_t.append(item["doc"])
        
        docs_Img = []
        for sitem in request_3:
            docs_Img.append(sitem[2])
        
        #将图片特征点匹配结果与前面已混合的文本检索,提取每页图片的存储位置做交集，存入docs_res 
        set_t = set(docs_t)
        set_Img = set(docs_Img)    
        docs_res = list(set_t & set_Img)
       
       
       #每页图的存储位置一定是一个唯一值，根据存储位置匹配数据库中的每一页的caption文本
        doc_texts = client.query(
                collection_name=self.collection_name,
                filter=f"doc in {docs_res} and customName in {customNames}",
                output_fields=["text","doc"],
                limit=len(docs_res),
            )    

        #构建model-rerank的请求，documents的索引顺序就是doc_texts里的顺序
        documents=[]
        for item in doc_texts:
            documents.append(item["text"])
        res = text_rerank(documents,query_param["text_query"],topk)
        
        #获取响应中的重排的索引顺序，匹配对应索引顺序的图片存储位置并返回
        search_output = []
        for res_item in res:
            search_output.append(doc_texts[res_item["index"]]["doc"])
        
        
        if(len(search_output) != topk):
            print(f"向量查询出现问题，不足topk:\n{search_output}")
        return search_output
    
    
    def Muti_hybrid_search_img_in_text(self,query_param, topk, rerank_topn=50, weights=(0.4, 0.3, 0.3)):
        # text semantic search (dense)
        customNames = query_param["customNames"]
        
        count = self.count_entity_customNames(customNames)
        if(count >= rerank_topn*2):
            rerank_topn = rerank_topn
        elif(count < rerank_topn*2 and count > topk*2):
            rerank_topn = count//2
        elif(count < topk*2 and count >= topk):
            rerank_topn = topk
        else:
            topk = count
            rerank_topn =count
        # print(f"topk:{topk}\nrerank_topn:{rerank_topn}\ncount:{count}\n")         
        search_param_1 = {
            "data": [query_param["text_dense_vector"]],
            "anns_field": "text_dense",
            "param": {"nprobe": 10},
            "expr": f"seq_id == 0 and customName in {customNames}",
            "limit": rerank_topn
        }
        request_1 = AnnSearchRequest(**search_param_1)
        
        # full-text search (sparse)
        search_param_2 = {
            "data": [query_param["text_query"]],
            "anns_field": "text_sparse",
            "param": {"drop_ratio_search": 0.2},
            "expr": f"seq_id == 0 and customName in {customNames}",
            "limit": rerank_topn
        }
        request_2 = AnnSearchRequest(**search_param_2)

        #先混合文本检索
        RRFranker = RRFRanker(100)
        reqs = [request_1,request_2]
        res = client.hybrid_search(
            collection_name=self.collection_name,
            reqs=reqs,
            ranker=RRFranker,
            limit=rerank_topn,
            output_fields=["doc","doc_id"]
        )
        doc_id=[]
        for resItem in res:
            for item in resItem:
                doc_id.append(item["doc_id"])
        request_3 = self.Img_search(query_param["image_query"],customNames,topk,doc_id)
        search_output = []
        for sitem in request_3:
            search_output.append(sitem[2])
        
        return search_output
        
    def Muti_hybrid_search_text_in_img(self,query_param, topk, rerank_topn=50, weights=(0.4, 0.3, 0.3)):
        customNames = query_param["customNames"]
        
        count = self.count_entity_customNames(customNames)
        if(count >= rerank_topn*2):
            rerank_topn = rerank_topn
        elif(count < rerank_topn*2 and count > topk*2):
            rerank_topn = count//2
        elif(count < topk*2 and count >= topk):
            rerank_topn = topk
        else:
            topk = count
            rerank_topn =count
        # print(f"topk:{topk}\nrerank_topn:{rerank_topn}\ncount:{count}\n")        
        request_3 = self.Img_search(query_param["image_query"],customNames,rerank_topn)
        doc = []
        for sitem in request_3:
            doc.append(sitem[2])
        
        # text semantic search (dense)
        search_param_1 = {
            "data": [query_param["text_dense_vector"]],
            "anns_field": "text_dense",
            "param": {"nprobe": 10},
            "expr": f"seq_id == 0 and customName in {customNames} and doc in {doc}",
            "limit": topk
        }
        request_1 = AnnSearchRequest(**search_param_1)
        
        # full-text search (sparse)
        search_param_2 = {
            "data": [query_param["text_query"]],
            "anns_field": "text_sparse",
            "param": {"drop_ratio_search": 0.2},
            "expr": f"seq_id == 0 and customName in {customNames} and doc in {doc}",
            "limit": topk
        }
        request_2 = AnnSearchRequest(**search_param_2)
        
        #混合文本检索
        RRFranker = RRFRanker(100)
        reqs = [request_1,request_2]
        res = client.hybrid_search(
            collection_name=self.collection_name,
            reqs=reqs,
            ranker=RRFranker,
            limit=topk,
            output_fields=["doc"]
        )
        
        search_output=[]
        for resItem in res:
            for item in resItem:
                search_output.append(item["doc"])
                     
        return search_output   

    def insert(self, data):
        # Insert ColBERT embeddings and metadata for a document into the collection.
        colbert_vecs = data["colbert_vecs"]
        seq_length = len(colbert_vecs)

       
        self.client.insert(
            self.collection_name,
            [
                {
                    "image_dense": colbert_vecs[i],
                    "seq_id": i,
                    "doc_id": data["doc_id"],
                    "doc": data["filepath"] if i == 0 else "",
                    "customName": data["customName"],
                    "text": data["text"] if i == 0 else "",
                    "text_dense": data["text_dense"] if i == 0 else ([0.0] * 1024)
                }
                for i in range(seq_length)
            ],
        )
        
    def delete_entity(self,customName):
        res = client.delete(
            collection_name=self.collection_name,
            filter=f"customName in ['{customName}']"
        )
        print(res)
        
    def delete_entity_all(self):
        client.drop_collection(collection_name=self.collection_name)
        
    def search_all_customName(self):
        res = client.query(
            collection_name=self.collection_name,
            filter="seq_id == 0 and doc_id == 0",
            output_fields=["customName"]
        )
        return res
    
    
    def count_entity(self):
        res = client.query(
            collection_name=self.collection_name,
            output_fields=["count(*)"]
        )
        return res
    
    def count_entity_customNames(self,customNames):
        res = client.query(
            collection_name=self.collection_name,
            filter=f"seq_id == 0 and customName in {customNames}",
            output_fields=["count(*)"]
        )
        count_value = res[0]['count(*)']
        return count_value
    
    def count_customNames_count(self):
        res = client.query(
            collection_name=self.collection_name,
            filter=f"seq_id == 0 and doc_id == 0",
            output_fields=["count(*)"]
        )
        count_value = res[0]['count(*)']
        return count_value